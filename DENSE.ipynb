{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, ModelCheckpoint\n",
    "import time\n",
    "import os\n",
    "from keras.engine import training_utils  \n",
    "from sklearn.utils import class_weight\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Defining main model characteristics:\n",
    "\n",
    "SEQ_LEN = 60  # how long of a preceeding sequence to collect \n",
    "FUTURE_PERIOD_PREDICT = 12  # how far into the future are we trying to predict\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "EPOCHS = 100  # how many passes through our data\n",
    "BATCH_SIZE = 128  # 64\n",
    "\n",
    "NNNAME = \"DENSE\"\n",
    "\n",
    "NAME = f\"{NNNAME}-{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"\n",
    "\n",
    "SAVE = True\n",
    "\n",
    "# Create models folder to save data in\n",
    "if not os.path.isdir('models'):\n",
    "    os.makedirs('models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# classify based on:\n",
    "#   * midpoint of best ask & best bid  later vs now\n",
    "#   * buy at best ask & sell at best bid\n",
    "#   * buy at last buy price & sell at last sell price\n",
    "#   * can make profitable trade from OB other side\n",
    "\n",
    "# changable:\n",
    "#   * threshold\n",
    "#   * how far into the future to comparedef classify(df):\n",
    "\n",
    "    change = 0.0015 # Safety factor in percentage\n",
    "    for i, row in df.iterrows():\n",
    "        target = 1\n",
    "        if df.at[i, \"future_best_bid\"] > df.at[i, \"best_ask\"] * (1 + change): # good to buy now\n",
    "            target = 2\n",
    "        elif df.at[i, \"future_best_ask\"] * (1 + change) < df.at[i, \"best_bid\"]: # good to sell now  \n",
    "            target = 0\n",
    "        df.at[i,\"target\"] = target\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three classes are calculated based on the change in return after the a certain time passed, with a threshold value of 0.0015 pecent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing the dataset to a uniform output format\n",
    "def preprocess(df, shorten=False):\n",
    "    df = df.loc[:,~df.columns.str.startswith('future_')]\n",
    "    \n",
    "    sprd = 2 * (df[\"best_ask\"] - df[\"best_bid\"]) / (df[\"best_ask\"] + df[\"best_bid\"])\n",
    "    df.insert(2, \"spread\", sprd)\n",
    "    \n",
    "    ob_maxsum = df.filter(regex=\"(a|b)\\d\").sum(axis=1).max()\n",
    "    ob_maxamt = df.filter(regex=\"last_buy_amt|last_sell_amt\").sum(axis=1).max()\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col in [\"target\", \"spread\"]:  # normalize all except for target, spread already done\n",
    "            continue\n",
    "        \n",
    "        if col in [\"best_ask\", \"best_bid\", \"last_buy_price\", \"last_sell_price\"]:\n",
    "            df[col] = df[col].pct_change()\n",
    "        elif col in [\"last_buy_amt\", \"last_sell_amt\"]:\n",
    "            df[col] = df[col] / ob_maxamt # preprocessing.scale(df[col].values)\n",
    "        else:\n",
    "            df[col] = df[col]/ob_maxsum\n",
    "            \n",
    "        df.dropna(inplace=True)  # remove the nas created by pct_change\n",
    "   \n",
    "    \n",
    "    df.dropna(inplace=True)  \n",
    "\n",
    "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
    "    # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
    "    prev_days = deque(maxlen=SEQ_LEN)\n",
    "    \n",
    "    for i in df.values:  # iterate over the values\n",
    "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
    "        if len(prev_days) == SEQ_LEN:  # make sure we have 60 sequences!\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])\n",
    "\n",
    "    buys = []  # list that will store our buy sequences and targets\n",
    "    sells = []  # list that will store our sell sequences and targets\n",
    "    holds = []\n",
    "\n",
    "    for seq, target in sequential_data:  # iterate over the sequential data\n",
    "        if target == 1:\n",
    "            holds.append([seq, target])\n",
    "        elif target == 2:\n",
    "            buys.append([seq, target])\n",
    "        elif target == 0:\n",
    "            sells.append([seq, target])\n",
    "\n",
    "    if shorten:\n",
    "        # make sure both lists are only up to the shortest length.\n",
    "        minlen = min(len(buys), len(sells), len(holds))\n",
    "        buys = buys[:minlen]\n",
    "        sells = sells[:minlen]\n",
    "        holds = holds[:minlen]\n",
    "\n",
    "\n",
    "    sequential_data = buys+sells+holds  # add them together\n",
    "    # another shuffle, so the model doesn't get confused with all 1 class then the other.\n",
    "    random.shuffle(sequential_data)\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for seq, target in sequential_data:\n",
    "        X.append(seq)  # X is the sequences\n",
    "        y.append(target)  # y is the targets/labels\n",
    "\n",
    "    return np.array(X), y \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_ask</th>\n",
       "      <th>best_bid</th>\n",
       "      <th>last_buy_price</th>\n",
       "      <th>last_buy_amt</th>\n",
       "      <th>last_sell_price</th>\n",
       "      <th>last_sell_amt</th>\n",
       "      <th>a0</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>...</th>\n",
       "      <th>b90</th>\n",
       "      <th>b91</th>\n",
       "      <th>b92</th>\n",
       "      <th>b93</th>\n",
       "      <th>b94</th>\n",
       "      <th>b95</th>\n",
       "      <th>b96</th>\n",
       "      <th>b97</th>\n",
       "      <th>b98</th>\n",
       "      <th>b99</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1585266597</td>\n",
       "      <td>6750.92</td>\n",
       "      <td>6748.04</td>\n",
       "      <td>6750.00</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>6752.40</td>\n",
       "      <td>0.004770</td>\n",
       "      <td>6.383364</td>\n",
       "      <td>1.487042</td>\n",
       "      <td>7.997782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003176</td>\n",
       "      <td>2.430000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.664000</td>\n",
       "      <td>0.924243</td>\n",
       "      <td>13.609199</td>\n",
       "      <td>0.006484</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1585266607</td>\n",
       "      <td>6749.51</td>\n",
       "      <td>6747.00</td>\n",
       "      <td>6745.51</td>\n",
       "      <td>0.006459</td>\n",
       "      <td>6750.32</td>\n",
       "      <td>0.005308</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.515601</td>\n",
       "      <td>0.503189</td>\n",
       "      <td>10.170660</td>\n",
       "      <td>...</td>\n",
       "      <td>2.430000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.664000</td>\n",
       "      <td>0.924243</td>\n",
       "      <td>13.609199</td>\n",
       "      <td>0.006484</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1585266617</td>\n",
       "      <td>6748.55</td>\n",
       "      <td>6746.32</td>\n",
       "      <td>6749.09</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>6749.07</td>\n",
       "      <td>0.008967</td>\n",
       "      <td>17.083271</td>\n",
       "      <td>2.912589</td>\n",
       "      <td>0.741189</td>\n",
       "      <td>1.031293</td>\n",
       "      <td>...</td>\n",
       "      <td>2.355000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.568243</td>\n",
       "      <td>0.078010</td>\n",
       "      <td>13.556673</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1585266627</td>\n",
       "      <td>6748.47</td>\n",
       "      <td>6748.46</td>\n",
       "      <td>6748.46</td>\n",
       "      <td>0.239064</td>\n",
       "      <td>6748.47</td>\n",
       "      <td>0.116225</td>\n",
       "      <td>0.008405</td>\n",
       "      <td>0.989471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273176</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>2.355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.568243</td>\n",
       "      <td>0.078010</td>\n",
       "      <td>13.556673</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>18.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1585266637</td>\n",
       "      <td>6750.00</td>\n",
       "      <td>6749.99</td>\n",
       "      <td>6748.46</td>\n",
       "      <td>0.239064</td>\n",
       "      <td>6750.00</td>\n",
       "      <td>0.021671</td>\n",
       "      <td>3.113139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.730782</td>\n",
       "      <td>1.397993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222206</td>\n",
       "      <td>0.273176</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.430000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.664000</td>\n",
       "      <td>0.982253</td>\n",
       "      <td>13.555673</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 206 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            best_ask  best_bid  last_buy_price  last_buy_amt  last_sell_price  \\\n",
       "time                                                                            \n",
       "1585266597   6750.92   6748.04         6750.00      0.000006          6752.40   \n",
       "1585266607   6749.51   6747.00         6745.51      0.006459          6750.32   \n",
       "1585266617   6748.55   6746.32         6749.09      0.300000          6749.07   \n",
       "1585266627   6748.47   6748.46         6748.46      0.239064          6748.47   \n",
       "1585266637   6750.00   6749.99         6748.46      0.239064          6750.00   \n",
       "\n",
       "            last_sell_amt         a0        a1        a2         a3  ...  \\\n",
       "time                                                                 ...   \n",
       "1585266597       0.004770   6.383364  1.487042  7.997782   0.000000  ...   \n",
       "1585266607       0.005308   0.002629  0.515601  0.503189  10.170660  ...   \n",
       "1585266617       0.008967  17.083271  2.912589  0.741189   1.031293  ...   \n",
       "1585266627       0.116225   0.008405  0.989471  0.000000   0.003189  ...   \n",
       "1585266637       0.021671   3.113139  0.000000  8.730782   1.397993  ...   \n",
       "\n",
       "                 b90       b91    b92       b93       b94        b95  \\\n",
       "time                                                                   \n",
       "1585266597  0.003176  2.430000  0.000  0.000000  2.664000   0.924243   \n",
       "1585266607  2.430000  0.000000  0.000  2.664000  0.924243  13.609199   \n",
       "1585266617  2.355000  0.000000  0.000  3.568243  0.078010  13.556673   \n",
       "1585266627  0.273176  0.075000  2.355  0.000000  0.000000   3.568243   \n",
       "1585266637  0.222206  0.273176  0.000  2.430000  0.000000   0.000000   \n",
       "\n",
       "                  b96        b97        b98     b99  \n",
       "time                                                 \n",
       "1585266597  13.609199   0.006484  18.500000   0.000  \n",
       "1585266607   0.006484  18.500000   0.000000   0.000  \n",
       "1585266617   0.001000  18.500000   0.000000   0.000  \n",
       "1585266627   0.078010  13.556673   0.001000  18.500  \n",
       "1585266637   2.664000   0.982253  13.555673   0.002  \n",
       "\n",
       "[5 rows x 206 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/Users/Ranykhalil/Downloads/Archive_2/\"\n",
    "\n",
    "data_file = \"obbinsandtrades_BTC-USD_2020-03-26-23.49.57.83_2020-04-03-15.49.47.83_17196840.csv\"\n",
    " \n",
    "def read_ob_csv(file):\n",
    "    df = pd.read_csv(path + file)\n",
    "    df.set_index(\"time\", inplace=True)\n",
    "    df_len = len(df.index.unique())\n",
    "    df_interval = df.index.unique()[1] - df.index.unique()[0]\n",
    "    df_init_t = df.index.unique()[0]\n",
    "    df_bins = len(df.loc[df.index == df_init_t].columns) - 6\n",
    "    return df_len, df_interval, df_init_t, df_bins, df\n",
    "\n",
    "df_len, df_interval, df_init_t, df_bins, df = read_ob_csv(data_file)\n",
    "\n",
    "df.fillna(method=\"ffill\", inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ SELL  HOLD  BUY ]\n",
      "[9056, 88077, 9415]\n"
     ]
    }
   ],
   "source": [
    "if \"future_best_ask\" not in df.columns:\n",
    "    for col in df.columns:\n",
    "        df['future_'+col] = df[col].shift(-FUTURE_PERIOD_PREDICT)\n",
    "    \n",
    "classify(df)\n",
    "df.dropna(inplace=True)\n",
    "print(\"[ SELL  HOLD  BUY ]\")\n",
    "print(df.groupby(\"target\").count()[\"best_ask\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape tr x (85176, 60, 207)\n",
      "shape tr y (85176,)\n",
      "shape val x (21246, 60, 207)\n",
      "shape val y (21246,)\n"
     ]
    }
   ],
   "source": [
    "#Data split non randomised\n",
    "times = sorted(df.index.values)\n",
    "validation_times = sorted(df.index.values)[-int(VALIDATION_SPLIT*len(times))]\n",
    "\n",
    "validation_df = df[(df.index >= validation_times)]\n",
    "df = df[(df.index < validation_times)]\n",
    "\n",
    "# Run data through preprocess function above\n",
    "train_x, train_y = preprocess(df, False)\n",
    "validation_x, validation_y = preprocess(validation_df, False)\n",
    "\n",
    "print(\"shape tr x\", np.shape(train_x))\n",
    "print(\"shape tr y\", np.shape(train_y))\n",
    "print(\"shape val x\", np.shape(validation_x))\n",
    "print(\"shape val y\", np.shape(validation_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we are looking to create in the following case is dense model with 5 layers with a final node count of three: hold, buy, sell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###\n",
      "### TRAIN      \t total: 85176 \t holds: 70239 \t buys: 7634 \t sells: 7303\n",
      "### VALIDATION \t total: 21246 \t holds: 17748 \t buys: 1774 \t sells: 1724\n",
      "###\n",
      "(85176, 60, 207)\n",
      "(60, 207)\n",
      "Train on 85176 samples, validate on 21246 samples\n",
      "Epoch 1/100\n",
      "85176/85176 [==============================] - 44s 521us/sample - loss: 1.1387 - sparse_categorical_accuracy: 0.4169 - val_loss: 0.9477 - val_sparse_categorical_accuracy: 0.6391\n",
      "Epoch 2/100\n",
      "85176/85176 [==============================] - 39s 461us/sample - loss: 0.9912 - sparse_categorical_accuracy: 0.4789 - val_loss: 1.1051 - val_sparse_categorical_accuracy: 0.3698\n",
      "Epoch 3/100\n",
      "85176/85176 [==============================] - 56s 656us/sample - loss: 0.8754 - sparse_categorical_accuracy: 0.5229 - val_loss: 0.9240 - val_sparse_categorical_accuracy: 0.5541\n",
      "Epoch 4/100\n",
      "85176/85176 [==============================] - 51s 593us/sample - loss: 0.7488 - sparse_categorical_accuracy: 0.5596 - val_loss: 1.1969 - val_sparse_categorical_accuracy: 0.4173\n",
      "Epoch 5/100\n",
      "85176/85176 [==============================] - 41s 486us/sample - loss: 0.6611 - sparse_categorical_accuracy: 0.6005 - val_loss: 1.5067 - val_sparse_categorical_accuracy: 0.2747\n",
      "Epoch 6/100\n",
      "85176/85176 [==============================] - 34s 394us/sample - loss: 0.5816 - sparse_categorical_accuracy: 0.6428 - val_loss: 1.4293 - val_sparse_categorical_accuracy: 0.3483\n",
      "Epoch 7/100\n",
      "85176/85176 [==============================] - 63s 737us/sample - loss: 0.5212 - sparse_categorical_accuracy: 0.6803 - val_loss: 1.3873 - val_sparse_categorical_accuracy: 0.4353\n",
      "Epoch 8/100\n",
      "85176/85176 [==============================] - 40s 465us/sample - loss: 0.4658 - sparse_categorical_accuracy: 0.7192 - val_loss: 1.2081 - val_sparse_categorical_accuracy: 0.5865\n",
      "Epoch 9/100\n",
      "85176/85176 [==============================] - 24s 282us/sample - loss: 0.4290 - sparse_categorical_accuracy: 0.7450 - val_loss: 1.4067 - val_sparse_categorical_accuracy: 0.4716\n",
      "Epoch 10/100\n",
      "85176/85176 [==============================] - 24s 285us/sample - loss: 0.3986 - sparse_categorical_accuracy: 0.7601 - val_loss: 1.2867 - val_sparse_categorical_accuracy: 0.5719\n",
      "Epoch 11/100\n",
      "85176/85176 [==============================] - 26s 304us/sample - loss: 0.3602 - sparse_categorical_accuracy: 0.7883 - val_loss: 1.2360 - val_sparse_categorical_accuracy: 0.5872\n",
      "Epoch 12/100\n",
      "85176/85176 [==============================] - 33s 382us/sample - loss: 0.3350 - sparse_categorical_accuracy: 0.8032 - val_loss: 1.3961 - val_sparse_categorical_accuracy: 0.5828\n",
      "Epoch 13/100\n",
      "85176/85176 [==============================] - 32s 378us/sample - loss: 0.3287 - sparse_categorical_accuracy: 0.8087 - val_loss: 1.4723 - val_sparse_categorical_accuracy: 0.5187\n",
      "Epoch 14/100\n",
      "85176/85176 [==============================] - 42s 490us/sample - loss: 0.2940 - sparse_categorical_accuracy: 0.8309 - val_loss: 1.5043 - val_sparse_categorical_accuracy: 0.5450\n",
      "Epoch 15/100\n",
      "85176/85176 [==============================] - 36s 424us/sample - loss: 0.2787 - sparse_categorical_accuracy: 0.8414 - val_loss: 1.3305 - val_sparse_categorical_accuracy: 0.6537\n",
      "Epoch 16/100\n",
      "85176/85176 [==============================] - 34s 398us/sample - loss: 0.2843 - sparse_categorical_accuracy: 0.8391 - val_loss: 1.5856 - val_sparse_categorical_accuracy: 0.5663\n",
      "Epoch 17/100\n",
      "85176/85176 [==============================] - 25s 298us/sample - loss: 0.2619 - sparse_categorical_accuracy: 0.8502 - val_loss: 1.4333 - val_sparse_categorical_accuracy: 0.6266\n",
      "Epoch 18/100\n",
      "85176/85176 [==============================] - 26s 308us/sample - loss: 0.2551 - sparse_categorical_accuracy: 0.8574 - val_loss: 1.6733 - val_sparse_categorical_accuracy: 0.5651\n",
      "Epoch 19/100\n",
      "85176/85176 [==============================] - 50s 586us/sample - loss: 0.2354 - sparse_categorical_accuracy: 0.8677 - val_loss: 1.6762 - val_sparse_categorical_accuracy: 0.5933\n",
      "Epoch 20/100\n",
      "85176/85176 [==============================] - 42s 492us/sample - loss: 0.2229 - sparse_categorical_accuracy: 0.8755 - val_loss: 1.6956 - val_sparse_categorical_accuracy: 0.5992\n",
      "Epoch 21/100\n",
      "85176/85176 [==============================] - 41s 479us/sample - loss: 0.2163 - sparse_categorical_accuracy: 0.8807 - val_loss: 1.5625 - val_sparse_categorical_accuracy: 0.5891\n",
      "Epoch 22/100\n",
      "85176/85176 [==============================] - 23s 272us/sample - loss: 0.2047 - sparse_categorical_accuracy: 0.8892 - val_loss: 1.6256 - val_sparse_categorical_accuracy: 0.5847\n",
      "Epoch 23/100\n",
      "85176/85176 [==============================] - 24s 276us/sample - loss: 0.2101 - sparse_categorical_accuracy: 0.8857 - val_loss: 1.6305 - val_sparse_categorical_accuracy: 0.6422\n",
      "Epoch 24/100\n",
      "85176/85176 [==============================] - 29s 342us/sample - loss: 0.1969 - sparse_categorical_accuracy: 0.8885 - val_loss: 1.9635 - val_sparse_categorical_accuracy: 0.5648\n",
      "Epoch 25/100\n",
      "85176/85176 [==============================] - 35s 412us/sample - loss: 0.1942 - sparse_categorical_accuracy: 0.8935 - val_loss: 1.6352 - val_sparse_categorical_accuracy: 0.6046\n",
      "Epoch 26/100\n",
      "85176/85176 [==============================] - 28s 332us/sample - loss: 0.1844 - sparse_categorical_accuracy: 0.9000 - val_loss: 1.8484 - val_sparse_categorical_accuracy: 0.6224\n",
      "Epoch 27/100\n",
      "85176/85176 [==============================] - 25s 292us/sample - loss: 0.1796 - sparse_categorical_accuracy: 0.9032 - val_loss: 1.6200 - val_sparse_categorical_accuracy: 0.6405\n",
      "Epoch 28/100\n",
      "85176/85176 [==============================] - 30s 353us/sample - loss: 0.1757 - sparse_categorical_accuracy: 0.9052 - val_loss: 1.7332 - val_sparse_categorical_accuracy: 0.6738\n",
      "Epoch 29/100\n",
      "85176/85176 [==============================] - 30s 347us/sample - loss: 0.1712 - sparse_categorical_accuracy: 0.9096 - val_loss: 1.5465 - val_sparse_categorical_accuracy: 0.6427\n",
      "Epoch 30/100\n",
      "85176/85176 [==============================] - 35s 405us/sample - loss: 0.1703 - sparse_categorical_accuracy: 0.9102 - val_loss: 1.5050 - val_sparse_categorical_accuracy: 0.6965\n",
      "Epoch 31/100\n",
      "85176/85176 [==============================] - 28s 325us/sample - loss: 0.1582 - sparse_categorical_accuracy: 0.9156 - val_loss: 1.7743 - val_sparse_categorical_accuracy: 0.6338\n",
      "Epoch 32/100\n",
      "85176/85176 [==============================] - 28s 330us/sample - loss: 0.1595 - sparse_categorical_accuracy: 0.9148 - val_loss: 1.8872 - val_sparse_categorical_accuracy: 0.6190\n",
      "Epoch 33/100\n",
      "85176/85176 [==============================] - 24s 277us/sample - loss: 0.1541 - sparse_categorical_accuracy: 0.9162 - val_loss: 1.6908 - val_sparse_categorical_accuracy: 0.6790\n",
      "Epoch 34/100\n",
      "85176/85176 [==============================] - 25s 291us/sample - loss: 0.1539 - sparse_categorical_accuracy: 0.9204 - val_loss: 1.6870 - val_sparse_categorical_accuracy: 0.6531\n",
      "Epoch 35/100\n",
      "85176/85176 [==============================] - 26s 307us/sample - loss: 0.1491 - sparse_categorical_accuracy: 0.9221 - val_loss: 1.7978 - val_sparse_categorical_accuracy: 0.6346\n",
      "Epoch 36/100\n",
      "85176/85176 [==============================] - 29s 339us/sample - loss: 0.1456 - sparse_categorical_accuracy: 0.9221 - val_loss: 1.7766 - val_sparse_categorical_accuracy: 0.6408\n",
      "Epoch 37/100\n",
      "85176/85176 [==============================] - 27s 319us/sample - loss: 0.1463 - sparse_categorical_accuracy: 0.9222 - val_loss: 1.9102 - val_sparse_categorical_accuracy: 0.6628\n",
      "Epoch 38/100\n",
      "85176/85176 [==============================] - 28s 329us/sample - loss: 0.1377 - sparse_categorical_accuracy: 0.9264 - val_loss: 1.6799 - val_sparse_categorical_accuracy: 0.6684\n",
      "Epoch 39/100\n",
      "85176/85176 [==============================] - 24s 284us/sample - loss: 0.1308 - sparse_categorical_accuracy: 0.9307 - val_loss: 1.5291 - val_sparse_categorical_accuracy: 0.7032\n",
      "Epoch 40/100\n",
      "85176/85176 [==============================] - 32s 381us/sample - loss: 0.1321 - sparse_categorical_accuracy: 0.9318 - val_loss: 1.8905 - val_sparse_categorical_accuracy: 0.6734\n",
      "Epoch 41/100\n",
      "85176/85176 [==============================] - 27s 313us/sample - loss: 0.1359 - sparse_categorical_accuracy: 0.9310 - val_loss: 1.7522 - val_sparse_categorical_accuracy: 0.6923\n",
      "Epoch 42/100\n",
      "85176/85176 [==============================] - 36s 424us/sample - loss: 0.1321 - sparse_categorical_accuracy: 0.9323 - val_loss: 1.6132 - val_sparse_categorical_accuracy: 0.6819\n",
      "Epoch 43/100\n",
      "85176/85176 [==============================] - 49s 576us/sample - loss: 0.1268 - sparse_categorical_accuracy: 0.9343 - val_loss: 1.6199 - val_sparse_categorical_accuracy: 0.7050\n",
      "Epoch 44/100\n",
      "85176/85176 [==============================] - 44s 520us/sample - loss: 0.1310 - sparse_categorical_accuracy: 0.9317 - val_loss: 1.8550 - val_sparse_categorical_accuracy: 0.6426\n",
      "Epoch 45/100\n",
      "85176/85176 [==============================] - 34s 398us/sample - loss: 0.1181 - sparse_categorical_accuracy: 0.9379 - val_loss: 1.7185 - val_sparse_categorical_accuracy: 0.6422\n",
      "Epoch 46/100\n",
      "85176/85176 [==============================] - 30s 346us/sample - loss: 0.1137 - sparse_categorical_accuracy: 0.9396 - val_loss: 1.7763 - val_sparse_categorical_accuracy: 0.6880\n",
      "Epoch 47/100\n",
      "85176/85176 [==============================] - 27s 312us/sample - loss: 0.1160 - sparse_categorical_accuracy: 0.9402 - val_loss: 1.7384 - val_sparse_categorical_accuracy: 0.6510\n",
      "Epoch 48/100\n",
      "85176/85176 [==============================] - 35s 407us/sample - loss: 0.1101 - sparse_categorical_accuracy: 0.9426 - val_loss: 1.7632 - val_sparse_categorical_accuracy: 0.6738\n",
      "Epoch 49/100\n",
      "85176/85176 [==============================] - 50s 584us/sample - loss: 0.1118 - sparse_categorical_accuracy: 0.9423 - val_loss: 1.7500 - val_sparse_categorical_accuracy: 0.6994\n",
      "Epoch 50/100\n",
      "85176/85176 [==============================] - 30s 357us/sample - loss: 0.1062 - sparse_categorical_accuracy: 0.9446 - val_loss: 1.8704 - val_sparse_categorical_accuracy: 0.6322\n",
      "Epoch 51/100\n",
      "85176/85176 [==============================] - 29s 338us/sample - loss: 0.1082 - sparse_categorical_accuracy: 0.9457 - val_loss: 1.7947 - val_sparse_categorical_accuracy: 0.6357\n",
      "Epoch 52/100\n",
      "85176/85176 [==============================] - 33s 385us/sample - loss: 0.1051 - sparse_categorical_accuracy: 0.9447 - val_loss: 1.8888 - val_sparse_categorical_accuracy: 0.6814\n",
      "Epoch 53/100\n",
      "85176/85176 [==============================] - 38s 450us/sample - loss: 0.1095 - sparse_categorical_accuracy: 0.9436 - val_loss: 1.7490 - val_sparse_categorical_accuracy: 0.6652\n",
      "Epoch 54/100\n",
      "85176/85176 [==============================] - 29s 337us/sample - loss: 0.1052 - sparse_categorical_accuracy: 0.9442 - val_loss: 1.7483 - val_sparse_categorical_accuracy: 0.7082\n",
      "Epoch 55/100\n",
      "85176/85176 [==============================] - 25s 294us/sample - loss: 0.0998 - sparse_categorical_accuracy: 0.9476 - val_loss: 1.8841 - val_sparse_categorical_accuracy: 0.7012\n",
      "Epoch 56/100\n",
      "85176/85176 [==============================] - 25s 296us/sample - loss: 0.1046 - sparse_categorical_accuracy: 0.9457 - val_loss: 1.7186 - val_sparse_categorical_accuracy: 0.7112\n",
      "Epoch 57/100\n",
      "85176/85176 [==============================] - 35s 414us/sample - loss: 0.1003 - sparse_categorical_accuracy: 0.9487 - val_loss: 1.6802 - val_sparse_categorical_accuracy: 0.7014\n",
      "Epoch 58/100\n",
      "85176/85176 [==============================] - 28s 331us/sample - loss: 0.0988 - sparse_categorical_accuracy: 0.9502 - val_loss: 1.7505 - val_sparse_categorical_accuracy: 0.6891\n",
      "Epoch 59/100\n",
      "85176/85176 [==============================] - 30s 350us/sample - loss: 0.0973 - sparse_categorical_accuracy: 0.9506 - val_loss: 2.0675 - val_sparse_categorical_accuracy: 0.6610\n",
      "Epoch 60/100\n",
      "85176/85176 [==============================] - 34s 405us/sample - loss: 0.1026 - sparse_categorical_accuracy: 0.9478 - val_loss: 2.2987 - val_sparse_categorical_accuracy: 0.6039\n",
      "Epoch 61/100\n",
      "85176/85176 [==============================] - 32s 375us/sample - loss: 0.0936 - sparse_categorical_accuracy: 0.9519 - val_loss: 1.9205 - val_sparse_categorical_accuracy: 0.6842\n",
      "Epoch 62/100\n",
      "85176/85176 [==============================] - 39s 462us/sample - loss: 0.0964 - sparse_categorical_accuracy: 0.9511 - val_loss: 2.1044 - val_sparse_categorical_accuracy: 0.6124\n",
      "Epoch 63/100\n",
      "85176/85176 [==============================] - 28s 329us/sample - loss: 0.0910 - sparse_categorical_accuracy: 0.9537 - val_loss: 1.8517 - val_sparse_categorical_accuracy: 0.7148\n",
      "Epoch 64/100\n",
      "85176/85176 [==============================] - 35s 410us/sample - loss: 0.0946 - sparse_categorical_accuracy: 0.9523 - val_loss: 1.9108 - val_sparse_categorical_accuracy: 0.6512\n",
      "Epoch 65/100\n",
      "85176/85176 [==============================] - 30s 355us/sample - loss: 0.0969 - sparse_categorical_accuracy: 0.9500 - val_loss: 2.1214 - val_sparse_categorical_accuracy: 0.6102\n",
      "Epoch 66/100\n",
      "85176/85176 [==============================] - 34s 396us/sample - loss: 0.0946 - sparse_categorical_accuracy: 0.9525 - val_loss: 2.0145 - val_sparse_categorical_accuracy: 0.6083\n",
      "Epoch 67/100\n",
      "85176/85176 [==============================] - 35s 412us/sample - loss: 0.0901 - sparse_categorical_accuracy: 0.9557 - val_loss: 1.9616 - val_sparse_categorical_accuracy: 0.6841\n",
      "Epoch 68/100\n",
      "85176/85176 [==============================] - 33s 393us/sample - loss: 0.0875 - sparse_categorical_accuracy: 0.9558 - val_loss: 2.0485 - val_sparse_categorical_accuracy: 0.6318\n",
      "Epoch 69/100\n",
      "85176/85176 [==============================] - 35s 407us/sample - loss: 0.0896 - sparse_categorical_accuracy: 0.9554 - val_loss: 1.8288 - val_sparse_categorical_accuracy: 0.6977\n",
      "Epoch 70/100\n",
      "85176/85176 [==============================] - 34s 399us/sample - loss: 0.0896 - sparse_categorical_accuracy: 0.9558 - val_loss: 1.7762 - val_sparse_categorical_accuracy: 0.7098\n",
      "Epoch 71/100\n",
      "85176/85176 [==============================] - 29s 336us/sample - loss: 0.0878 - sparse_categorical_accuracy: 0.9572 - val_loss: 1.8571 - val_sparse_categorical_accuracy: 0.7157\n",
      "Epoch 72/100\n",
      "85176/85176 [==============================] - 28s 334us/sample - loss: 0.0841 - sparse_categorical_accuracy: 0.9594 - val_loss: 1.7547 - val_sparse_categorical_accuracy: 0.6785\n",
      "Epoch 73/100\n",
      "85176/85176 [==============================] - 27s 314us/sample - loss: 0.0849 - sparse_categorical_accuracy: 0.9566 - val_loss: 1.8503 - val_sparse_categorical_accuracy: 0.7558\n",
      "Epoch 74/100\n",
      "85176/85176 [==============================] - 35s 408us/sample - loss: 0.0790 - sparse_categorical_accuracy: 0.9610 - val_loss: 2.0991 - val_sparse_categorical_accuracy: 0.6674\n",
      "Epoch 75/100\n",
      "85176/85176 [==============================] - 31s 368us/sample - loss: 0.0767 - sparse_categorical_accuracy: 0.9612 - val_loss: 1.9628 - val_sparse_categorical_accuracy: 0.6966\n",
      "Epoch 76/100\n",
      "85176/85176 [==============================] - 28s 333us/sample - loss: 0.0835 - sparse_categorical_accuracy: 0.9582 - val_loss: 1.9050 - val_sparse_categorical_accuracy: 0.6963\n",
      "Epoch 77/100\n",
      "85176/85176 [==============================] - 29s 341us/sample - loss: 0.0827 - sparse_categorical_accuracy: 0.9577 - val_loss: 1.9583 - val_sparse_categorical_accuracy: 0.6932\n",
      "Epoch 78/100\n",
      "85176/85176 [==============================] - 25s 293us/sample - loss: 0.0801 - sparse_categorical_accuracy: 0.9599 - val_loss: 2.0437 - val_sparse_categorical_accuracy: 0.6850\n",
      "Epoch 79/100\n",
      "85176/85176 [==============================] - 31s 362us/sample - loss: 0.0776 - sparse_categorical_accuracy: 0.9612 - val_loss: 2.0954 - val_sparse_categorical_accuracy: 0.6613\n",
      "Epoch 80/100\n",
      "85176/85176 [==============================] - 30s 349us/sample - loss: 0.0792 - sparse_categorical_accuracy: 0.9611 - val_loss: 2.0338 - val_sparse_categorical_accuracy: 0.6569\n",
      "Epoch 81/100\n",
      "85176/85176 [==============================] - 34s 398us/sample - loss: 0.0759 - sparse_categorical_accuracy: 0.9633 - val_loss: 1.8375 - val_sparse_categorical_accuracy: 0.6990\n",
      "Epoch 82/100\n",
      "85176/85176 [==============================] - 28s 332us/sample - loss: 0.0746 - sparse_categorical_accuracy: 0.9642 - val_loss: 2.0086 - val_sparse_categorical_accuracy: 0.6897\n",
      "Epoch 83/100\n",
      "85176/85176 [==============================] - 24s 282us/sample - loss: 0.0787 - sparse_categorical_accuracy: 0.9614 - val_loss: 2.3341 - val_sparse_categorical_accuracy: 0.6191\n",
      "Epoch 84/100\n",
      "85176/85176 [==============================] - 27s 317us/sample - loss: 0.0797 - sparse_categorical_accuracy: 0.9602 - val_loss: 1.9389 - val_sparse_categorical_accuracy: 0.7207\n",
      "Epoch 85/100\n",
      "85176/85176 [==============================] - 27s 322us/sample - loss: 0.0791 - sparse_categorical_accuracy: 0.9617 - val_loss: 2.0303 - val_sparse_categorical_accuracy: 0.6632\n",
      "Epoch 86/100\n",
      "85176/85176 [==============================] - 23s 264us/sample - loss: 0.0751 - sparse_categorical_accuracy: 0.9624 - val_loss: 1.9516 - val_sparse_categorical_accuracy: 0.6565\n",
      "Epoch 87/100\n",
      "85176/85176 [==============================] - 24s 287us/sample - loss: 0.0728 - sparse_categorical_accuracy: 0.9619 - val_loss: 2.0817 - val_sparse_categorical_accuracy: 0.6814\n",
      "Epoch 88/100\n",
      "85176/85176 [==============================] - 24s 282us/sample - loss: 0.0695 - sparse_categorical_accuracy: 0.9653 - val_loss: 1.8897 - val_sparse_categorical_accuracy: 0.7446\n",
      "Epoch 89/100\n",
      "85176/85176 [==============================] - 25s 290us/sample - loss: 0.0679 - sparse_categorical_accuracy: 0.9666 - val_loss: 2.0701 - val_sparse_categorical_accuracy: 0.7038\n",
      "Epoch 90/100\n",
      "85176/85176 [==============================] - 22s 264us/sample - loss: 0.0741 - sparse_categorical_accuracy: 0.9630 - val_loss: 2.0621 - val_sparse_categorical_accuracy: 0.6718\n",
      "Epoch 91/100\n",
      "85176/85176 [==============================] - 22s 263us/sample - loss: 0.0755 - sparse_categorical_accuracy: 0.9624 - val_loss: 1.9007 - val_sparse_categorical_accuracy: 0.7481\n",
      "Epoch 92/100\n",
      "85176/85176 [==============================] - 23s 270us/sample - loss: 0.0708 - sparse_categorical_accuracy: 0.9667 - val_loss: 2.6609 - val_sparse_categorical_accuracy: 0.5549\n",
      "Epoch 93/100\n",
      "85176/85176 [==============================] - 20s 238us/sample - loss: 0.0726 - sparse_categorical_accuracy: 0.9654 - val_loss: 2.1797 - val_sparse_categorical_accuracy: 0.6248\n",
      "Epoch 94/100\n",
      "85176/85176 [==============================] - 25s 289us/sample - loss: 0.0710 - sparse_categorical_accuracy: 0.9651 - val_loss: 2.2702 - val_sparse_categorical_accuracy: 0.6412\n",
      "Epoch 95/100\n",
      "85176/85176 [==============================] - 22s 263us/sample - loss: 0.0688 - sparse_categorical_accuracy: 0.9654 - val_loss: 1.8650 - val_sparse_categorical_accuracy: 0.7156\n",
      "Epoch 96/100\n",
      "85176/85176 [==============================] - 21s 245us/sample - loss: 0.0727 - sparse_categorical_accuracy: 0.9650 - val_loss: 2.0213 - val_sparse_categorical_accuracy: 0.7109\n",
      "Epoch 97/100\n",
      "85176/85176 [==============================] - 30s 347us/sample - loss: 0.0700 - sparse_categorical_accuracy: 0.9663 - val_loss: 1.9709 - val_sparse_categorical_accuracy: 0.7149\n",
      "Epoch 98/100\n",
      "85176/85176 [==============================] - 24s 278us/sample - loss: 0.0664 - sparse_categorical_accuracy: 0.9674 - val_loss: 1.9244 - val_sparse_categorical_accuracy: 0.7517\n",
      "Epoch 99/100\n",
      "85176/85176 [==============================] - 24s 278us/sample - loss: 0.0756 - sparse_categorical_accuracy: 0.9659 - val_loss: 2.0469 - val_sparse_categorical_accuracy: 0.6761\n",
      "Epoch 100/100\n",
      "85176/85176 [==============================] - 22s 261us/sample - loss: 0.0662 - sparse_categorical_accuracy: 0.9687 - val_loss: 1.9565 - val_sparse_categorical_accuracy: 0.7059\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"###\n",
    "### TRAIN      \\t total: {len(train_x)} \\t holds: {train_y.count(1)} \\t buys: {train_y.count(2)} \\t sells: {train_y.count(0)}\n",
    "### VALIDATION \\t total: {len(validation_x)} \\t holds: {validation_y.count(1)} \\t buys: {validation_y.count(2)} \\t sells: {validation_y.count(0)}\n",
    "###\"\"\")\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_x.shape[1:])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(train_x.shape[1:])))\n",
    "\n",
    "n_cells = 256 # 128\n",
    "do_layer  = 0.2\n",
    "\n",
    "model.add(Dense(n_cells, activation='relu'))\n",
    "model.add(Dropout(do_layer))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(n_cells, activation='relu'))\n",
    "model.add(Dropout(do_layer))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(n_cells, activation='relu'))\n",
    "model.add(Dropout(do_layer))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    "# Saving the model ----------------------------\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME)) \n",
    "# unique file name that will include the epoch and the validation acc for that epoch\n",
    "filepath = NNNAME + \"-{epoch:02d}-{val_sparse_categorical_accuracy:.3f}\"\n",
    "checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='val_sparse_categorical_accuracy',\n",
    "                                                   verbose=1, save_best_only=True, mode='max'))  # saves only the best ones\n",
    "# -------------------------\n",
    "\n",
    "# Find the balanced class weights\n",
    "train_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(train_y),\n",
    "                                                 train_y)\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    train_x, train_y,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(validation_x, validation_y),\n",
    "    callbacks=[tensorboard, checkpoint],\n",
    "    class_weight=dict(enumerate(train_weights)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After all the epoch runs the validation accuracy is found to be 0.7 while the train data was at 0.9 indicating an overfit model but even with this large difference a validation score of 0.7 is considered a relatively high number especially comparing it to the logistic regression of the baseline model that only included a positive, negative accuracy calculation found to be 0.54\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will try looking at the model run through an LSTM (Long Short Term Memory) model. LSTM networks used to classify, process and make predictions based on time series data, since there can be lags of unknown duration between important events in a time series. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('learn-env': conda)",
   "language": "python",
   "name": "python36964bitlearnenvconda73a6b7ead5a84093afcb9dca25787081"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
